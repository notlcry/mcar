
# 📝 项目需求报告：基于树莓派4B的AI语音桌宠小车

---

## 一、项目概述

### 1.1 项目名称

> **语音互动AI桌宠小车**

### 1.2 项目背景

你已经基于 Raspberry Pi 4B 制作了一个配有**云台摄像头**、**红外避障**和**基础移动控制系统**的小车，本项目计划在此基础上拓展**语音交互、AI对话、表情动画、语音合成**等功能，使之成为一台具备**人格化、感知力与表达力的智能桌宠机器人**。

### 1.3 项目目标

* 实现语音唤醒、语音识别、语音对话
* 接入 AI 大模型（如 ChatGPT）进行自然语言交互
* 通过小屏幕/OLED 显示“表情”和“状态”
* 支持 TTS 合成语音播报回答
* 联动云台/轮子进行“情绪表现”或物理互动
* 实现“AI 桌宠+移动终端”的融合体验

---

## 二、系统架构设计

### 2.1 硬件组成

| 模块             | 说明                   |
| -------------- | -------------------- |
| 树莓派 4B         | 系统主控，运行 Python 程序与模型 |
| USB 麦克风        | 输入用户语音               |
| 扬声器（3.5mm或USB） | 播放语音回答               |
| OLED / TFT 屏   | 显示“眼睛”或“脸部表情”        |
| 云台摄像头          | 实现“看向你”、跟随等功能        |
| 红外避障模块         | 实现自动避障与导航判断          |
| 电机驱动 + 小车底盘    | 实现基础前进、后退、转向等移动能力    |

---

### 2.2 软件模块

```plaintext
主程序 main.py
│
├── 唤醒词监听模块（porcupine）
├── 语音识别模块（whisper.cpp或Whisper API）
├── 自然语言理解模块（ChatGPT API）
├── 语音合成模块（edge-tts）
├── 动作控制模块（车体运动 / 云台旋转）
├── 表情渲染模块（OLED 屏幕动画）
└── 状态管理 / 情感系统（状态机 + 情绪判断）
```

---

## 三、功能需求明细

### 3.1 基础语音交互功能

| 功能    | 描述                               |
| ----- | -------------------------------- |
| 唤醒词检测 | 识别如“快快”唤醒后开始对话（使用 Porcupine）   |
| 语音转文本 | 用户说话后转换为文字（使用 Whisper.cpp 或 API） |
| 文本对话  | 发送用户文本到大模型（如 GPT-3.5/4）并获取回复     |
| 语音播报  | 使用 edge-tts 播放语音回答内容             |
| 表情联动  | 回答时配合“张嘴”“眨眼”等动画表现               |

---

### 3.2 AI 桌宠个性表现

| 功能    | 描述                         |
| ----- | -------------------------- |
| 状态感知  | 根据回答内容识别情绪（开心、生气、疑惑等）      |
| 表情变化  | 不同状态切换不同 OLED 动画           |
| 动作反馈  | 根据情绪驱动小车动作，如“兴奋时转圈”        |
| 云台联动  | 说话时云台转向用户方向（可选人脸跟踪）        |
| 个性化语言 | 支持“我的名字叫快快，我喜欢唱歌”等拟人化对话训练 |

---

### 3.3 小车行为联动

| 触发场景    | 小车行为          |
| ------- | ------------- |
| “我们走吧！” | 小车前进          |
| “小车转个圈” | 小车原地旋转一圈      |
| “你生气了吗” | 小车快速后退 + 红灯表情 |
| “你很开心嘛” | 小车左右晃动 + 爱心表情 |

---

### 3.4 扩展能力（非必需但可预留）

* 记忆系统：记住常见问题或用户偏好
* 人脸识别：识别来访者并自动打招呼
* 本地数据库：离线状态下仍能执行部分对话/控制
* Wi-Fi 配网 / OTA 升级

---

## 四、非功能性需求

| 类别   | 要求                              |
| ---- | ------------------------------- |
| 响应时间 | 唤醒后能在3秒内完成一次完整对话交互              |
| 性能需求 | 程序在树莓派4B运行流畅，占用CPU不超过80%        |
| 离线能力 | 尽可能将 ASR 本地化（Whisper.cpp），减少云依赖 |
| 安全性  | 避免发送敏感数据到 API，大模型接口应设限          |
| 易扩展性 | 所有模块松耦合，便于后续接入视觉或导航算法           |

---

## 五、开发任务与时间计划（初步建议）

| 时间  | 开发任务                         |
| --- | ---------------------------- |
| 第1周 | 安装并测试 Whisper.cpp + edge-tts |
| 第2周 | 接入 ChatGPT API，完成语音对话链路      |
| 第3周 | OLED屏幕驱动与表情渲染开发              |
| 第4周 | 动作/情绪联动逻辑初步接入                |
| 第5周 | 小车运动控制集成（统一到主逻辑）             |
| 第6周 | 调试、优化响应速度与体验，拍摄展示视频          |

---

## 六、可选技术栈与工具

| 功能   | 推荐工具                         |
| ---- | ---------------------------- |
| ASR  | whisper.cpp / OpenAI Whisper |
| 大模型  | OpenAI GPT / 通义千问 API        |
| TTS  | edge-tts（微软） / pyttsx3（离线）   |
| 表情动画 | pygame / PIL + SPI 显示驱动      |
| 云台控制 | 使用已有串口协议或 GPIO 控制            |
| 小车运动 | 使用 GPIO + 电机驱动板（如 L298N）     |
| 系统管理 | supervisord / systemd 启动桌宠服务 |

---

## 七、风险与对策

| 风险      | 对策                               |
| ------- | -------------------------------- |
| 网络不稳定   | 支持本地离线唤醒和命令词控制                   |
| 云端接口变动  | 所有 API 均封装为接口层，便于替换              |
| 性能瓶颈    | 使用 whisper.cpp 的 tiny 模型，TTS异步播放 |
| 系统崩溃或卡死 | 加 watchdog 和冷重启机制                |

---

## 八、预期成果展示形式

* AI 桌宠介绍短视频（会说话、能转头、有性格）
* GitHub 项目源码 + 软硬件接线图
* 可复现的开发文档与用户说明书
* 后续可发展为教学产品或 STEM 课程内容
